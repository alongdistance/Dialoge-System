{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from linguaf import descriptive_statistics as ds\n",
    "from linguaf import syntactical_complexity as sc\n",
    "from linguaf import lexical_diversity as ld\n",
    "import spacy\n",
    "import textdescriptives as td\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from spacy.lang.en import English # updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"Salt is used on roads to help melt ice and snow and improve traction during the winter months. When it's cold outside, water can freeze on the roads and make them very slippery, which can be dangerous for cars and people. Salt helps to melt the ice and snow by lowering the freezing point of water, which means that it can help keep the roads clear and safe to travel on. \\nThere are other options for melting ice and snow on roads, such as using chemicals like calcium chloride or magnesium chloride, or using mechanical methods like plows or sand. However, salt is often the most effective and affordable option for many communities, especially when it's used in combination with other methods. \\nIt's important to note that while salt can be helpful for making roads safer during the winter, it can also have negative effects on the environment and on the cars themselves. Salt can cause corrosion on metal surfaces, including cars, and it can also harm plants and animals if it washes into nearby waterways. However, despite these potential downsides, many communities continue to use salt as a way to keep roads clear and safe during the winter.\"\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.text.strip() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Salt is used on roads to help melt ice and snow and improve traction during the winter months.',\n",
       " \"When it's cold outside, water can freeze on the roads and make them very slippery, which can be dangerous for cars and people.\",\n",
       " 'Salt helps to melt the ice and snow by lowering the freezing point of water, which means that it can help keep the roads clear and safe to travel on.',\n",
       " 'There are other options for melting ice and snow on roads, such as using chemicals like calcium chloride or magnesium chloride, or using mechanical methods like plows or sand.',\n",
       " \"However, salt is often the most effective and affordable option for many communities, especially when it's used in combination with other methods.\",\n",
       " \"It's important to note that while salt can be helpful for making roads safer during the winter, it can also have negative effects on the environment and on the cars themselves.\",\n",
       " 'Salt can cause corrosion on metal surfaces, including cars, and it can also harm plants and animals if it washes into nearby waterways.',\n",
       " 'However, despite these potential downsides, many communities continue to use salt as a way to keep roads clear and safe during the winter.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\n",
    "sentiment_task(\"T'estimo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86e6255f9c34266aef1026246e1ce43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39.7524)\n",
      "tensor(37.4699)\n",
      "tensor(100.7684)\n",
      "tensor(78.5179)\n",
      "tensor(35.7740)\n",
      "tensor(45.2173)\n",
      "tensor(52.8712)\n",
      "tensor(100.6129)\n",
      "tensor(33.3190)\n",
      "tensor(103.2431)\n",
      "tensor(7.6517)\n",
      "tensor(12.6544)\n",
      "tensor(9.2697)\n",
      "tensor(13.7105)\n",
      "tensor(9.3818)\n",
      "tensor(7.5883)\n",
      "tensor(9.3689)\n",
      "tensor(8.4582)\n",
      "tensor(8.6495)\n",
      "tensor(8.7933)\n"
     ]
    }
   ],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "text = \"Salt is used on roads to help melt ice and snow and improve traction during the winter months. When it's cold outside, water can freeze on the roads and make them very slippery, which can be dangerous for cars and people. Salt helps to melt the ice and snow by lowering the freezing point of water, which means that it can help keep the roads clear and safe to travel on. \\nThere are other options for melting ice and snow on roads, such as using chemicals like calcium chloride or magnesium chloride, or using mechanical methods like plows or sand. However, salt is often the most effective and affordable option for many communities, especially when it's used in combination with other methods. \\nIt's important to note that while salt can be helpful for making roads safer during the winter, it can also have negative effects on the environment and on the cars themselves. Salt can cause corrosion on metal surfaces, including cars, and it can also harm plants and animals if it washes into nearby waterways. However, despite these potential downsides, many communities continue to use salt as a way to keep roads clear and safe during the winter.\"\n",
    "text = \"Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\"\n",
    "\n",
    "for text in human_document[:10]:\n",
    "    \n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = encoded_input['input_ids']\n",
    "    target_ids = encoded_input['input_ids'].clone()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, labels=target_ids)\n",
    "        print(torch.exp(output.loss))\n",
    "\n",
    "for text in chatgpt_document[:10]:\n",
    "    \n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = encoded_input['input_ids']\n",
    "    target_ids = encoded_input['input_ids'].clone()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, labels=target_ids)\n",
    "        print(torch.exp(output.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_input['input_ids']\n",
    "target_ids = encoded_input['input_ids'].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = model(input_ids, labels=target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8857, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8857, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 287644])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"Hello-SimpleAI/HC3\", \"all\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<textdescriptives.components.dependency_distance.DependencyDistance at 0x7f91f8eadc30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(\"textdescriptives/dependency_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data['train']['chatgpt_answers'][:100]\n",
    "chatgpt_document = [sentence for sublist in tmp for sentence in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Salt is used on roads to help melt ice and snow and improve traction during the winter months. When it's cold outside, water can freeze on the roads and make them very slippery, which can be dangerous for cars and people. Salt helps to melt the ice and snow by lowering the freezing point of water, which means that it can help keep the roads clear and safe to travel on. \\nThere are other options for melting ice and snow on roads, such as using chemicals like calcium chloride or magnesium chloride, or using mechanical methods like plows or sand. However, salt is often the most effective and affordable option for many communities, especially when it's used in combination with other methods. \\nIt's important to note that while salt can be helpful for making roads safer during the winter, it can also have negative effects on the environment and on the cars themselves. Salt can cause corrosion on metal surfaces, including cars, and it can also harm plants and animals if it washes into nearby waterways. However, despite these potential downsides, many communities continue to use salt as a way to keep roads clear and safe during the winter.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_document[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data['train']['human_answers'][:100]\n",
    "human_document = [sentence for sublist in tmp for sentence in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp(chatgpt_document[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'neutral', 'score': 0.45749813318252563}]\n",
      "[{'label': 'neutral', 'score': 0.4027189314365387}]\n",
      "[{'label': 'neutral', 'score': 0.43154311180114746}]\n",
      "[{'label': 'negative', 'score': 0.6339156627655029}]\n",
      "[{'label': 'neutral', 'score': 0.40721404552459717}]\n",
      "[{'label': 'neutral', 'score': 0.5306320786476135}]\n",
      "[{'label': 'negative', 'score': 0.46121907234191895}]\n",
      "[{'label': 'negative', 'score': 0.49167028069496155}]\n",
      "[{'label': 'neutral', 'score': 0.4284146726131439}]\n",
      "[{'label': 'negative', 'score': 0.45517289638519287}]\n",
      "[{'label': 'negative', 'score': 0.43077778816223145}]\n",
      "[{'label': 'neutral', 'score': 0.5234262347221375}]\n",
      "[{'label': 'neutral', 'score': 0.38644132018089294}]\n",
      "[{'label': 'neutral', 'score': 0.5572836995124817}]\n",
      "[{'label': 'negative', 'score': 0.44156718254089355}]\n",
      "[{'label': 'neutral', 'score': 0.5262146592140198}]\n",
      "[{'label': 'negative', 'score': 0.42967304587364197}]\n",
      "[{'label': 'negative', 'score': 0.5718052983283997}]\n",
      "[{'label': 'neutral', 'score': 0.44277849793434143}]\n",
      "[{'label': 'neutral', 'score': 0.49161437153816223}]\n",
      "[{'label': 'neutral', 'score': 0.4425106942653656}]\n",
      "[{'label': 'neutral', 'score': 0.4536520540714264}]\n",
      "[{'label': 'neutral', 'score': 0.467519074678421}]\n",
      "[{'label': 'neutral', 'score': 0.4463289976119995}]\n",
      "[{'label': 'neutral', 'score': 0.4428742229938507}]\n",
      "[{'label': 'positive', 'score': 0.4111312925815582}]\n",
      "[{'label': 'neutral', 'score': 0.4137613773345947}]\n",
      "[{'label': 'neutral', 'score': 0.4581111669540405}]\n",
      "[{'label': 'neutral', 'score': 0.5258015394210815}]\n",
      "[{'label': 'negative', 'score': 0.4378992021083832}]\n",
      "[{'label': 'neutral', 'score': 0.5778335332870483}]\n",
      "[{'label': 'neutral', 'score': 0.4184266924858093}]\n",
      "[{'label': 'neutral', 'score': 0.5489044785499573}]\n",
      "[{'label': 'negative', 'score': 0.5815356373786926}]\n",
      "[{'label': 'neutral', 'score': 0.4671650826931}]\n",
      "[{'label': 'neutral', 'score': 0.5646960735321045}]\n",
      "[{'label': 'neutral', 'score': 0.47554197907447815}]\n",
      "[{'label': 'neutral', 'score': 0.4602274000644684}]\n",
      "[{'label': 'neutral', 'score': 0.5372331738471985}]\n",
      "[{'label': 'neutral', 'score': 0.39019596576690674}]\n",
      "[{'label': 'neutral', 'score': 0.4631083607673645}]\n",
      "[{'label': 'neutral', 'score': 0.592136561870575}]\n",
      "[{'label': 'negative', 'score': 0.5441229343414307}]\n",
      "[{'label': 'neutral', 'score': 0.4197794497013092}]\n",
      "[{'label': 'neutral', 'score': 0.47772476077079773}]\n",
      "[{'label': 'neutral', 'score': 0.4730830192565918}]\n",
      "[{'label': 'neutral', 'score': 0.39815855026245117}]\n",
      "[{'label': 'negative', 'score': 0.5235493183135986}]\n",
      "[{'label': 'negative', 'score': 0.41975533962249756}]\n",
      "[{'label': 'negative', 'score': 0.6154101490974426}]\n",
      "[{'label': 'negative', 'score': 0.6072290539741516}]\n",
      "[{'label': 'neutral', 'score': 0.5610253214836121}]\n",
      "[{'label': 'neutral', 'score': 0.5100710988044739}]\n",
      "[{'label': 'neutral', 'score': 0.5720518827438354}]\n",
      "[{'label': 'neutral', 'score': 0.45300236344337463}]\n",
      "[{'label': 'neutral', 'score': 0.4410112798213959}]\n",
      "[{'label': 'neutral', 'score': 0.4997153580188751}]\n",
      "[{'label': 'neutral', 'score': 0.5202508568763733}]\n",
      "[{'label': 'neutral', 'score': 0.45812752842903137}]\n",
      "[{'label': 'negative', 'score': 0.4878734350204468}]\n",
      "[{'label': 'neutral', 'score': 0.5569103956222534}]\n",
      "[{'label': 'neutral', 'score': 0.47971832752227783}]\n",
      "[{'label': 'negative', 'score': 0.428156316280365}]\n",
      "[{'label': 'neutral', 'score': 0.45894894003868103}]\n",
      "[{'label': 'neutral', 'score': 0.4908140003681183}]\n",
      "[{'label': 'negative', 'score': 0.4368401765823364}]\n",
      "[{'label': 'neutral', 'score': 0.4258666932582855}]\n",
      "[{'label': 'neutral', 'score': 0.4708893895149231}]\n",
      "[{'label': 'neutral', 'score': 0.4140925705432892}]\n",
      "[{'label': 'negative', 'score': 0.4533265233039856}]\n",
      "[{'label': 'neutral', 'score': 0.49051883816719055}]\n",
      "[{'label': 'negative', 'score': 0.7980594038963318}]\n",
      "[{'label': 'neutral', 'score': 0.45533013343811035}]\n",
      "[{'label': 'negative', 'score': 0.5341027975082397}]\n",
      "[{'label': 'neutral', 'score': 0.4454940855503082}]\n",
      "[{'label': 'neutral', 'score': 0.5333666205406189}]\n",
      "[{'label': 'neutral', 'score': 0.48640742897987366}]\n",
      "[{'label': 'negative', 'score': 0.43700286746025085}]\n",
      "[{'label': 'neutral', 'score': 0.41915568709373474}]\n",
      "[{'label': 'negative', 'score': 0.7076361775398254}]\n",
      "[{'label': 'neutral', 'score': 0.4842534363269806}]\n",
      "[{'label': 'neutral', 'score': 0.5720327496528625}]\n",
      "[{'label': 'neutral', 'score': 0.4219966530799866}]\n",
      "[{'label': 'neutral', 'score': 0.4559524655342102}]\n",
      "[{'label': 'negative', 'score': 0.6491549015045166}]\n",
      "[{'label': 'neutral', 'score': 0.48435163497924805}]\n",
      "[{'label': 'negative', 'score': 0.4004019796848297}]\n",
      "[{'label': 'negative', 'score': 0.4664650559425354}]\n",
      "[{'label': 'neutral', 'score': 0.43837353587150574}]\n",
      "[{'label': 'neutral', 'score': 0.4970547556877136}]\n",
      "[{'label': 'positive', 'score': 0.3872244656085968}]\n",
      "[{'label': 'negative', 'score': 0.4188610911369324}]\n",
      "[{'label': 'neutral', 'score': 0.44155973196029663}]\n",
      "[{'label': 'neutral', 'score': 0.3952288329601288}]\n",
      "[{'label': 'neutral', 'score': 0.46016108989715576}]\n",
      "[{'label': 'neutral', 'score': 0.43201348185539246}]\n",
      "[{'label': 'neutral', 'score': 0.5051718950271606}]\n",
      "[{'label': 'neutral', 'score': 0.5949217081069946}]\n",
      "[{'label': 'negative', 'score': 0.4851188361644745}]\n",
      "[{'label': 'neutral', 'score': 0.41459181904792786}]\n"
     ]
    }
   ],
   "source": [
    "for answer in chatgpt_document[:100]:\n",
    "    print(sentiment_task(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.4892612397670746}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_task(human_document[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "piano_text = chatgpt_document[1]\n",
    "piano_doc = nlp(piano_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Salt is used on roads to help melt ice and snow and improve traction during the winter months. When it's cold outside, water can freeze on the roads and make them very slippery, which can be dangerous for cars and people. Salt helps to melt the ice and snow by lowering the freezing point of water, which means that it can help keep the roads clear and safe to travel on. \\nThere are other options for melting ice and snow on roads, such as using chemicals like calcium chloride or magnesium chloride, or using mechanical methods like plows or sand. However, salt is often the most effective and affordable option for many communities, especially when it's used in combination with other methods. \\nIt's important to note that while salt can be helpful for making roads safer during the winter, it can also have negative effects on the environment and on the cars themselves. Salt can cause corrosion on metal surfaces, including cars, and it can also harm plants and animals if it washes into nearby waterways. However, despite these potential downsides, many communities continue to use salt as a way to keep roads clear and safe during the winter.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piano_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Salt\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'nsubjpass'\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'auxpass'\n",
      "\n",
      "TOKEN: used\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =0\n",
      "token.tag_ = 'VBN'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: on\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: roads\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'on'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: to\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'TO'\n",
      "token.head.text = 'help'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: help\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'xcomp'\n",
      "\n",
      "TOKEN: melt\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'help'\n",
      "token.dep_ = 'xcomp'\n",
      "\n",
      "TOKEN: ice\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'melt'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'ice'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: snow\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'ice'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'melt'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: improve\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =5\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'melt'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: traction\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'improve'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: during\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'improve'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'months'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: winter\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'months'\n",
      "token.dep_ = 'compound'\n",
      "\n",
      "TOKEN: months\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'during'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =16\n",
      "token.tag_ = '.'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: When\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'WRB'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: it\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: 's\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =6\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'advcl'\n",
      "\n",
      "TOKEN: cold\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'acomp'\n",
      "\n",
      "TOKEN: outside\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'RB'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = ','\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: water\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: can\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'MD'\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: freeze\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =0\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: on\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'roads'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: roads\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'on'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: make\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =5\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: them\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = 'slippery'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: very\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'RB'\n",
      "token.head.text = 'slippery'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: slippery\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'make'\n",
      "token.dep_ = 'ccomp'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = ','\n",
      "token.head.text = 'slippery'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: which\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'WDT'\n",
      "token.head.text = 'be'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: can\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'MD'\n",
      "token.head.text = 'be'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: be\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'slippery'\n",
      "token.dep_ = 'advcl'\n",
      "\n",
      "TOKEN: dangerous\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'be'\n",
      "token.dep_ = 'acomp'\n",
      "\n",
      "TOKEN: for\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'dangerous'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: cars\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'for'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'cars'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: people\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'cars'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =18\n",
      "token.tag_ = '.'\n",
      "token.head.text = 'freeze'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: Salt\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'helps'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: helps\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =0\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = 'helps'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: to\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'TO'\n",
      "token.head.text = 'melt'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: melt\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'helps'\n",
      "token.dep_ = 'xcomp'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'ice'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: ice\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'melt'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'ice'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: snow\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'ice'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: by\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =5\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'melt'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: lowering\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'VBG'\n",
      "token.head.text = 'by'\n",
      "token.dep_ = 'pcomp'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'point'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: freezing\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'point'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: point\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'lowering'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: of\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'point'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: water\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'of'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = ','\n",
      "token.head.text = 'point'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: which\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'WDT'\n",
      "token.head.text = 'means'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: means\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =5\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = 'point'\n",
      "token.dep_ = 'relcl'\n",
      "\n",
      "TOKEN: that\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'help'\n",
      "token.dep_ = 'mark'\n",
      "\n",
      "TOKEN: it\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = 'help'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: can\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'MD'\n",
      "token.head.text = 'help'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: help\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'means'\n",
      "token.dep_ = 'ccomp'\n",
      "\n",
      "TOKEN: keep\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'help'\n",
      "token.dep_ = 'xcomp'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'roads'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: roads\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'keep'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: clear\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'keep'\n",
      "token.dep_ = 'oprd'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'clear'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: safe\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'clear'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: to\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'TO'\n",
      "token.head.text = 'travel'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: travel\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =7\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'keep'\n",
      "token.dep_ = 'xcomp'\n",
      "\n",
      "TOKEN: on\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'travel'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =30\n",
      "token.tag_ = '.'\n",
      "token.head.text = 'helps'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: \n",
      "\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = '_SP'\n",
      "token.head.text = '.'\n",
      "token.dep_ = 'dep'\n",
      "\n",
      "TOKEN: There\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'EX'\n",
      "token.head.text = 'are'\n",
      "token.dep_ = 'expl'\n",
      "\n",
      "TOKEN: are\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =0\n",
      "token.tag_ = 'VBP'\n",
      "token.head.text = 'are'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: other\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'options'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: options\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'are'\n",
      "token.dep_ = 'attr'\n",
      "\n",
      "TOKEN: for\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'options'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: melting\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'VBG'\n",
      "token.head.text = 'for'\n",
      "token.dep_ = 'pcomp'\n",
      "\n",
      "TOKEN: ice\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'melting'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'ice'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: snow\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'ice'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: on\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'melting'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: roads\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'on'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = ','\n",
      "token.head.text = 'roads'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: such\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'as'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: as\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'roads'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: using\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'VBG'\n",
      "token.head.text = 'as'\n",
      "token.dep_ = 'pcomp'\n",
      "\n",
      "TOKEN: chemicals\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'using'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: like\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'chemicals'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: calcium\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'chloride'\n",
      "token.dep_ = 'compound'\n",
      "\n",
      "TOKEN: chloride\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'like'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: or\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'chloride'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: magnesium\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'chloride'\n",
      "token.dep_ = 'compound'\n",
      "\n",
      "TOKEN: chloride\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'chloride'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =8\n",
      "token.tag_ = ','\n",
      "token.head.text = 'using'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: or\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =9\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'using'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: using\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =10\n",
      "token.tag_ = 'VBG'\n",
      "token.head.text = 'using'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: mechanical\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'methods'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: methods\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'using'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: like\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'methods'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: plows\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'like'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: or\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'plows'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: sand\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'plows'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =30\n",
      "token.tag_ = '.'\n",
      "token.head.text = 'are'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: However\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'RB'\n",
      "token.head.text = 'is'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = ','\n",
      "token.head.text = 'is'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: salt\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'is'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =0\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = 'is'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: often\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'RB'\n",
      "token.head.text = 'is'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =5\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'option'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: most\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'RBS'\n",
      "token.head.text = 'effective'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: effective\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'option'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'effective'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: affordable\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'effective'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: option\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =7\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'is'\n",
      "token.dep_ = 'attr'\n",
      "\n",
      "TOKEN: for\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'option'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: many\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'communities'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: communities\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'for'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = ','\n",
      "token.head.text = 'option'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: especially\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = 'RB'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: when\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'WRB'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: it\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'nsubjpass'\n",
      "\n",
      "TOKEN: 's\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'auxpass'\n",
      "\n",
      "TOKEN: used\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =9\n",
      "token.tag_ = 'VBN'\n",
      "token.head.text = 'option'\n",
      "token.dep_ = 'relcl'\n",
      "\n",
      "TOKEN: in\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'used'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: combination\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'in'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: with\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'combination'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: other\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'methods'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: methods\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'with'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =22\n",
      "token.tag_ = '.'\n",
      "token.head.text = 'is'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: \n",
      "\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = '_SP'\n",
      "token.head.text = '.'\n",
      "token.dep_ = 'dep'\n",
      "\n",
      "TOKEN: It\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: 's\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =0\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: important\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'acomp'\n",
      "\n",
      "TOKEN: to\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'TO'\n",
      "token.head.text = 'note'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: note\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'xcomp'\n",
      "\n",
      "TOKEN: that\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =17\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'have'\n",
      "token.dep_ = 'mark'\n",
      "\n",
      "TOKEN: while\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'be'\n",
      "token.dep_ = 'mark'\n",
      "\n",
      "TOKEN: salt\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'be'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: can\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'MD'\n",
      "token.head.text = 'be'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: be\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =13\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'have'\n",
      "token.dep_ = 'advcl'\n",
      "\n",
      "TOKEN: helpful\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'be'\n",
      "token.dep_ = 'acomp'\n",
      "\n",
      "TOKEN: for\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'helpful'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: making\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'VBG'\n",
      "token.head.text = 'for'\n",
      "token.dep_ = 'pcomp'\n",
      "\n",
      "TOKEN: roads\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'safer'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: safer\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'JJR'\n",
      "token.head.text = 'making'\n",
      "token.dep_ = 'ccomp'\n",
      "\n",
      "TOKEN: during\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'making'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'winter'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: winter\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'during'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = ','\n",
      "token.head.text = 'have'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: it\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = 'have'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: can\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'MD'\n",
      "token.head.text = 'have'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: also\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'RB'\n",
      "token.head.text = 'have'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: have\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =18\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'note'\n",
      "token.dep_ = 'ccomp'\n",
      "\n",
      "TOKEN: negative\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'effects'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: effects\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'have'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: on\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'effects'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'environment'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: environment\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'on'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'on'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: on\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =4\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'on'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'cars'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: cars\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'on'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: themselves\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = 'cars'\n",
      "token.dep_ = 'appos'\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =32\n",
      "token.tag_ = '.'\n",
      "token.head.text = \"'s\"\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: Salt\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'cause'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: can\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'MD'\n",
      "token.head.text = 'cause'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: cause\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =0\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'cause'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: corrosion\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'cause'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: on\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'corrosion'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: metal\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'surfaces'\n",
      "token.dep_ = 'compound'\n",
      "\n",
      "TOKEN: surfaces\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'on'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = ','\n",
      "token.head.text = 'surfaces'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: including\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'VBG'\n",
      "token.head.text = 'surfaces'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: cars\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'including'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =8\n",
      "token.tag_ = ','\n",
      "token.head.text = 'cause'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =9\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'cause'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: it\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = 'harm'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: can\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'MD'\n",
      "token.head.text = 'harm'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: also\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'RB'\n",
      "token.head.text = 'harm'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: harm\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =13\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'cause'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: plants\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'harm'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'plants'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: animals\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'plants'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: if\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'washes'\n",
      "token.dep_ = 'mark'\n",
      "\n",
      "TOKEN: it\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'PRP'\n",
      "token.head.text = 'washes'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: washes\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =6\n",
      "token.tag_ = 'VBZ'\n",
      "token.head.text = 'harm'\n",
      "token.dep_ = 'advcl'\n",
      "\n",
      "TOKEN: into\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'washes'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: nearby\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'waterways'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: waterways\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'into'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =10\n",
      "token.tag_ = '.'\n",
      "token.head.text = 'harm'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: However\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =9\n",
      "token.tag_ = 'RB'\n",
      "token.head.text = 'continue'\n",
      "token.dep_ = 'advmod'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =8\n",
      "token.tag_ = ','\n",
      "token.head.text = 'continue'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: despite\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =7\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'continue'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: these\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'downsides'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: potential\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'downsides'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: downsides\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'despite'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: ,\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =3\n",
      "token.tag_ = ','\n",
      "token.head.text = 'continue'\n",
      "token.dep_ = 'punct'\n",
      "\n",
      "TOKEN: many\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'communities'\n",
      "token.dep_ = 'amod'\n",
      "\n",
      "TOKEN: communities\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'continue'\n",
      "token.dep_ = 'nsubj'\n",
      "\n",
      "TOKEN: continue\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =0\n",
      "token.tag_ = 'VBP'\n",
      "token.head.text = 'continue'\n",
      "token.dep_ = 'ROOT'\n",
      "\n",
      "TOKEN: to\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'TO'\n",
      "token.head.text = 'use'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: use\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'continue'\n",
      "token.dep_ = 'xcomp'\n",
      "\n",
      "TOKEN: salt\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'use'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: as\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'use'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'way'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: way\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'as'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: to\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'TO'\n",
      "token.head.text = 'keep'\n",
      "token.dep_ = 'aux'\n",
      "\n",
      "TOKEN: keep\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'VB'\n",
      "token.head.text = 'way'\n",
      "token.dep_ = 'relcl'\n",
      "\n",
      "TOKEN: roads\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'NNS'\n",
      "token.head.text = 'keep'\n",
      "token.dep_ = 'dobj'\n",
      "\n",
      "TOKEN: clear\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'keep'\n",
      "token.dep_ = 'oprd'\n",
      "\n",
      "TOKEN: and\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'CC'\n",
      "token.head.text = 'clear'\n",
      "token.dep_ = 'cc'\n",
      "\n",
      "TOKEN: safe\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'JJ'\n",
      "token.head.text = 'clear'\n",
      "token.dep_ = 'conj'\n",
      "\n",
      "TOKEN: during\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =5\n",
      "token.tag_ = 'IN'\n",
      "token.head.text = 'keep'\n",
      "token.dep_ = 'prep'\n",
      "\n",
      "TOKEN: the\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =1\n",
      "token.tag_ = 'DT'\n",
      "token.head.text = 'winter'\n",
      "token.dep_ = 'det'\n",
      "\n",
      "TOKEN: winter\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =2\n",
      "token.tag_ = 'NN'\n",
      "token.head.text = 'during'\n",
      "token.dep_ = 'pobj'\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "token._.dependency_distance[\"dependency_distance\"] =16\n",
      "token.tag_ = '.'\n",
      "token.head.text = 'continue'\n",
      "token.dep_ = 'punct'\n"
     ]
    }
   ],
   "source": [
    "for token in piano_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {token.text}\n",
    "=====\n",
    "{token._.dependency_distance[\"dependency_distance\"] =}\n",
    "{token.tag_ = }\n",
    "{token.head.text = }\n",
    "{token.dep_ = }\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dependency_distance': 16, 'adjacent_dependency': False}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token._.dependency_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt: 103.46355353075171\n",
      "human: 83.47422680412372\n"
     ]
    }
   ],
   "source": [
    "print(f\"chatgpt: {ds.avg_sentence_length(chatgpt_document)}\")\n",
    "print(f\"human: {ds.avg_sentence_length(human_document)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt: 2.796281193932474\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (56.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "print(f\"chatgpt: {sc.mean_dependency_distance(chatgpt_document)}\")\n",
    "print(f\"human: {sc.mean_dependency_distance(human_document)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module linguaf.syntactical_complexity in linguaf:\n",
      "\n",
      "NAME\n",
      "    linguaf.syntactical_complexity\n",
      "\n",
      "FUNCTIONS\n",
      "    mean_dependency_distance(documents: list, lang: str = 'en') -> float\n",
      "        Calculates Mean Dependency Distance score over a list of documents\n",
      "        The higher the score the more complex are the sentences.\n",
      "        \n",
      "        Keyword arguments:\n",
      "        documents -- the list of textual documents\n",
      "        lang -- language of the textual documents\n",
      "\n",
      "FILE\n",
      "    /Users/zhangxiwen/miniconda3/envs/test/lib/python3.10/site-packages/linguaf/syntactical_complexity.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.63636363636364"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.avg_sentence_length(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9041382922996335"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.mean_dependency_distance(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.83825738607912"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld.lexical_density(document)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
